{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d1fdd8-9e16-48af-96c3-7135d455164b",
   "metadata": {},
   "source": [
    "### Implementing Self-supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a3546",
   "metadata": {},
   "source": [
    "**Tutorial 8.1. To implement self-training classifier on Iris Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff26d0e-df64-42eb-95bd-ac88cda42f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/semi_supervised/_self_training.py:212: UserWarning: y contains no unlabeled samples\n",
      "  warnings.warn(\"y contains no unlabeled samples\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the Iris dataset (labeled data)\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split data into labeled and unlabeled portions\n",
    "X_labeled, X_unlabeled, y_labeled, _ = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\n",
    "# Initialize a base classifier (e.g., logistic regression)\n",
    "base_classifier = LogisticRegression()\n",
    "\n",
    "# Create a self-training classifier\n",
    "self_training_clf = SelfTrainingClassifier(base_classifier)\n",
    "\n",
    "# Fit the model using labeled data\n",
    "self_training_clf.fit(X_labeled, y_labeled)\n",
    "\n",
    "# Predict on unlabeled data\n",
    "y_pred_unlabeled = self_training_clf.predict(X_unlabeled)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f26e0",
   "metadata": {},
   "source": [
    "**Tutorial 8.2. To implement self-training classifier on Iris Dataset**\n",
    "Finally, evaluate the performance of your self-training classifier using appropriate metrics (e.g., accuracy, F1-score, etc.). You can compare it with the performance of the base classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98fa361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Assuming y_unlabeled_true contains true labels for unlabeled data\n",
    "accuracy = accuracy_score(y_unlabeled_true, y_pred_unlabeled)\n",
    "f1 = f1_score(y_unlabeled_true, y_pred_unlabeled, average='weighted')\n",
    "precision = precision_score(y_unlabeled_true, y_pred_unlabeled, average='weighted')\n",
    "recall = recall_score(y_unlabeled_true, y_pred_unlabeled, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note: Calibration of the classifier is important for better results. Calibration of the classifier is essential for better results. You can fine-tune hyperparameters or use techniques like Platt scaling or isotonic regression to improve calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb5007",
   "metadata": {},
   "source": [
    "**Tutorial 8.2. To implement word embeddings using self-supervised task using Word2Vec method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50866e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Load a corpus (e.g., Brown corpus)\n",
    "sentences = brown.sents()\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Get word embeddings\n",
    "vector_king = model.wv['king']\n",
    "vector_queen = model.wv['queen']\n",
    "\n",
    "# Similarity between words\n",
    "similarity = model.wv.similarity('king', 'queen')\n",
    "\n",
    "print(f\"Vector similarity between 'king' and 'queen': {similarity:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
