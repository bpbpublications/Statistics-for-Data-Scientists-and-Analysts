{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3413b86-5611-4480-be09-64c4bea69a6e",
   "metadata": {},
   "source": [
    "ing\n",
    "o Fitting models to independent data\n",
    " Linear regr ession\n",
    " Logistic reg ression\n",
    "o Fitting models to depend ent data\n",
    " Multilevel linear regressi on models\n",
    " Multilevel logistic regress ion models\n",
    " Marginal linear regression\n",
    " Marginal logisti c regression\n",
    "o Bayesi an approaches\n",
    "o Decision tree and random forest\n",
    "o Support vector machine\n",
    "• Unsup ervised learn ing\n",
    "o Clu stering\n",
    " K-m eans\n",
    " K-prototype\n",
    "o Hie rarchical clustering\n",
    "o Ga ussian mixture models\n",
    "o Princi pal component analysis\n",
    "o Singu• r value decomposition\n",
    "o Modelselection and evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d0a44-3f28-4d3b-957b-f8fa58f57346",
   "metadata": {},
   "source": [
    "**Tutorial 7.1: To implement and illustrate the concept of fitting models to independent data**\n",
    "\n",
    "The algorithm used to fit the model is linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f572a-7222-4adc-b095-8c0c47ff5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Define the data\n",
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # Number of hours studied\n",
    "y = np.array([50, 60, 65, 70, 75, 80, 85, 90, 95, 100])  # Test score\n",
    "# Fit the linear regression model\n",
    "m, b = np.polyfit(x, y, 1)  # Find the slope and the intercept\n",
    "# Print the results\n",
    "print(f\"The slope of the line is {m:.2f}\")\n",
    "print(f\"The intercept of the line is {b:.2f}\")\n",
    "print(f\"The equation of the line is y = {m:.2f}x + {b:.2f}\")\n",
    "# Plot the data and the line\n",
    "# Data represent the actual values of the number of hours studied and the test score for each student\n",
    "# Line represents the linear regression model that predicts the test score based on the number of hours studied\n",
    "plt.scatter(x, y, color=\"blue\", label=\"Data\")  # Plot the data points\n",
    "plt.plot(x, m*x + b, color=\"red\",\n",
    "         label=\"Linear regression model\")  # Plot the line\n",
    "plt.xlabel(\"Number of hours studied\")  # Label the x-axis\n",
    "plt.ylabel(\"Test score\")  # Label the y-axis\n",
    "plt.legend()  # Show the legend\n",
    "plt.savefig('fitting_models_to_independent_data.jpg',\n",
    "            dpi=600, bbox_inches='tight')  # Show the figure\n",
    "plt.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5016b2a-dd59-4275-bad3-8b9ed5289129",
   "metadata": {},
   "source": [
    "**Tutorial 7.2: To implement and illustrate the concept of linear regression models to fit a model to predict house price based on size and location**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1f8ed1-53d9-4d1f-8c07-4ec2237f0922",
   "metadata": {},
   "source": [
    "**_Syntax for implementing linear regression with sklearn_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64fa3c3-0824-4e91-86a6-b01efe850fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Create a linear regression model\n",
    "linear_regression = LinearRegression()\n",
    "# Train the model\n",
    "linear_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d1b61-81cb-4e2c-91c2-c95a5aa1035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sklearn linear regression library\n",
    "import sklearn.linear_model as lm\n",
    "# Create some fake data\n",
    "# Size and location of the houses\n",
    "x = [[50, 1], [60, 2], [70, 3], [80, 4], [90, 5]]\n",
    "y = [100, 120, 140, 160, 180]  # Price of the houses\n",
    "# Create a linear regression model\n",
    "model = lm.LinearRegression()\n",
    "# Fit the model to the data\n",
    "model.fit(x, y)\n",
    "# Print the intercept (b) and the slope (w1 and w2)\n",
    "print(f\"Intercept: {model.intercept_}\")  # b\n",
    "print(f\"Coefficient/Slope: {model.coef_}\")  # w1 and w2\n",
    "# Predict the price of a house with size 75 and location 3\n",
    "print(f\"Prediction: {model.predict([[75, 3]])}\")  # y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1718011-f4f3-48a9-80cc-0f0b02b335b0",
   "metadata": {},
   "source": [
    "**Tutorial 7.3: To visualize the fitted line in above tutorial 7.2 and the data points in a scatter plot**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab39cb8-f79a-42e9-aa5e-8259dfa24165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Extract the x and y values from the data\n",
    "x_values = [row[0] for row in x]\n",
    "y_values = y\n",
    "# Plot the data points as a scatter plot\n",
    "plt.scatter(x_values, y_values, color=\"blue\", label=\"Data points\")\n",
    "# Plot the fitted line as a line plot\n",
    "plt.plot(x_values, model.predict(x), color=\"red\",\n",
    "         label=\"Fitted linear regression model\")\n",
    "# Add some labels and a legend\n",
    "plt.xlabel(\"Size of the house\")\n",
    "plt.ylabel(\"Price of the house\")\n",
    "plt.legend()\n",
    "plt.savefig('house_price_prediction_model.jpg', dpi=600,\n",
    "            bbox_inches='tight')  # Show the figure\n",
    "plt.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3871f47-6ea1-45a0-b0df-ff9234fe27ca",
   "metadata": {},
   "source": [
    "**Tutorial 7.4: To show a scatter plot where data follow curved pattern**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7db95-3f0d-42a4-b43e-34cd89e4fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Some data that follows a curved pattern\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.sin(x)\n",
    "# Plot the data as a scatter plot\n",
    "plt.scatter(x, y, color='blue', label='Data')\n",
    "# Fit a polynomial curve to the data\n",
    "p = np.polyfit(x, y, 6)\n",
    "y_fit = np.polyval(p, x)\n",
    "# Plot the curve as a red line\n",
    "plt.plot(x, y_fit, color='red', label='Curve')\n",
    "# Add some labels and a legend\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "# Save the figure\n",
    "plt.savefig('scatter_curve.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d1c17d-246d-4a8a-8797-c60a918ef10b",
   "metadata": {},
   "source": [
    "**Tutorial 7.5. To implement viewing of the linearity (linear pattern) in the data by plotting the data in a scatterplot**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7dddb-b609-4750-a6e5-e019815b0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Define the x and y variables\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "y = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "# Create a scatter plot\n",
    "plt.scatter(x, y, color=\"red\", marker=\"o\")\n",
    "# Add labels and title\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Linear relationship between x and y\")\n",
    "# Save the figure\n",
    "plt.savefig('linearity.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a6a0c-074a-4902-ba14-65a63fd75748",
   "metadata": {},
   "source": [
    "**Tutorial 7.6: To check the normality of data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd0883-6648-4ff4-932b-de68afd13541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "# Define data\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "y = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "# Fit a linear regression model using OLS\n",
    "model = sm.OLS(y, x).fit()  # Create and fit an OLS object\n",
    "# Get the predicted values\n",
    "y_pred = model.predict()\n",
    "# Calculate the residuals\n",
    "residuals = y - y_pred\n",
    "# Plot the residuals\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "# Save the figure\n",
    "plt.savefig('normality.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126253b-ca9e-4e7b-a221-819a80825ad4",
   "metadata": {},
   "source": [
    "**Tutorial 7.7. To implement logistic regression to predict whether a student will pass an exam based on the number of hours they studied**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d20c3ab-e374-4d8e-99e7-ec4828020c59",
   "metadata": {},
   "source": [
    "**_Syntax for implementing logistic regression with sklearn_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc53f8c0-f197-4b92-8ac4-2fc2e46d78fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Create a logistic regression model\n",
    "logistic_regression = LogisticRegression()\n",
    "# Train the model\n",
    "logistic_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884237f8-5c86-41a4-9859-ca017c89f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import libraries from sklearn for logistic regression prediction\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Create the data\n",
    "data = {\"Hours studied\": [0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5],\n",
    "        \"Passed\": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]}\n",
    "df = pd.DataFrame(data)\n",
    "# Define the independent and dependent variables\n",
    "X = df[\"Hours studied\"].values.reshape(-1, 1)\n",
    "y = df[\"Passed\"].values\n",
    "# Fit the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "# Predict the probabilities for different values of hours studied\n",
    "x_new = np.linspace(0, 6, 100).reshape(-1, 1)\n",
    "y_new = model.predict_proba(x_new)[:, 1]\n",
    "# Take the number of hours 3.76 to predict the probability of passing\n",
    "x_fixed = 3.76\n",
    "# Predict the probability of passing for the fixed number of hours\n",
    "y_fixed = model.predict_proba([[x_fixed]])[0, 1]\n",
    "# Print the fixed number of hours and the predicted probability\n",
    "print(f\"The fixed number of hours : {x_fixed:.2f}\")\n",
    "print(f\"The predicted probability of passing : {y_fixed:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62126c-5d1b-46ef-8f24-c7bbfd83c17c",
   "metadata": {},
   "source": [
    "**Tutorial 7.8. To visualize above logistic regression model to predict whether a student will pass an exam based on the number of hours they studied in a plot**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9a482-9970-48e7-8eaf-3ac7208fa2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the data and the logistic regression curve\n",
    "plt.scatter(X, y, color=\"blue\", label=\"Data\")\n",
    "plt.plot(x_new, y_new, color=\"red\", label=\"Logistic regression model\")\n",
    "plt.xlabel(\"Hours studied\")\n",
    "plt.ylabel(\"Probability of passing\")\n",
    "plt.legend()\n",
    "# Show the figure\n",
    "plt.savefig('student_reasult_prediction_model.jpg',\n",
    "            dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa93c040-5478-414b-9666-e500c808e73a",
   "metadata": {},
   "source": [
    "**Tutorial 7.9. To implement linear mixed effect model to predict blood pressure from 10 patients**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73a5de-3afc-44ed-a12e-f4de125262a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "# Generate some dummy data\n",
    "import numpy as np\n",
    "np.random.seed(50)\n",
    "n_patients = 10  # Number of patients\n",
    "n_obs = 5  # Number of observations per patient\n",
    "x = np.random.randn(n_patients * n_obs)  # Covariate\n",
    "patient = np.repeat(np.arange(n_patients), n_obs)  # Patient ID\n",
    "bp = 100 + 5 * x + 10 * np.random.randn(n_patients * n_obs)  # Blood pressure\n",
    "# Create a data frame\n",
    "df = pd.DataFrame({\"bp\": bp, \"x\": x, \"patient\": patient})\n",
    "# Fit a linear mixed effect model with a random intercept for each patient\n",
    "model = sm.MixedLM.from_formula(\"bp ~ x\", groups=\"patient\", data=df)\n",
    "result = model.fit()\n",
    "# Print the summary\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ecf97-d610-4cc9-a7b9-8d30dd3ff262",
   "metadata": {},
   "source": [
    "**Tutorial 7.10. To implement a decision tree algorithm on patient data to classify the blood pressure of 20 patients into `low`,`normal`,`high`**\n",
    "\n",
    "Dataset used: `patient_data.csv` Synthetic dataset generated for this book.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6865056b-5674-488d-9b18-27e74aaa22ab",
   "metadata": {},
   "source": [
    "**_Syntax for implementing decision tree with sklearn_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66c9ee-d849-40fb-a0d2-feca0bfad5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Create a decision tree classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "# Train the classifier\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff0578-66c0-41f7-a557-3983215752fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Read the data\n",
    "data = pd.read_csv(\n",
    "    \"/workspaces/ImplementingStatisticsWithPython/data/chapter7/patient_data.csv\")\n",
    "# Separate the features and the target\n",
    "X = data.drop(\"blood_pressure\", axis=1)\n",
    "y = data[\"blood_pressure\"]\n",
    "# Encode the categorical features\n",
    "X[\"gender\"] = X[\"gender\"].map({\"M\": 0, \"F\": 1})\n",
    "# Build and train the decision tree\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ef74f-222e-4e50-bc65-93231bbaceda",
   "metadata": {},
   "source": [
    "**Tutorial 7.11. To view graphical representation of the decision tree, showing the features, thresholds, impurity, and class labels at each node**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679248b-76f2-420d-98db-320646dcb494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Import the plot_tree function from the sklearn.tree module\n",
    "from sklearn.tree import plot_tree\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Fill the nodes with colors, round the corners, and add feature and class names\n",
    "plot_tree(tree, filled=True, rounded=True, feature_names=X.columns,\n",
    "          class_names=[\"Low\", \"Normal\", \"High\"], fontsize=12)\n",
    "# Show the figure\n",
    "plt.savefig('decision_tree.jpg', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc618cc9-c24a-4365-bb41-75ba0f497abe",
   "metadata": {},
   "source": [
    "**Tutorial 7.12: To implement decision tree by including the separation of dependent and independent variables, train_test split and then fitting data on train set, based on Tutorial 7.10**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407c8cc-db60-4fec-b505-9c40971e5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import the accuracy_score function\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Read the data\n",
    "data = pd.read_csv(\n",
    "    \"/workspaces/ImplementingStatisticsWithPython/data/chapter7/patient_data.csv\")\n",
    "# Separate the features and the target\n",
    "X = data.drop(\"blood_pressure\", axis=1)  # independent variables\n",
    "y = data[\"blood_pressure\"]  # dependent variable\n",
    "# Encode the categorical features\n",
    "X[\"gender\"] = X[\"gender\"].map({\"M\": 0, \"F\": 1})\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "# Build and train the decision tree on the training set\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "# Further test set can be used to evaluate the model\n",
    "# Predict the values for the test set\n",
    "y_pred = tree.predict(X_test)  # Get the predicted values for the test data\n",
    "# Calculate the accuracy score on the test set\n",
    "# Compare the predicted values with the actual values\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy of the decision tree model on the test set :\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997df9c1-adb2-4db4-adc7-72dbc6268f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is Random Forest ?\n",
    "2. How it works ?\n",
    "3. When or where is Random Forest useful, In what kind of data and situation ? \n",
    "4. Give an example to explain Random Forest? Use the same example to explain Random Forest to a eigth grader ?\n",
    "5. How is Random Forest implement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62241e6-7b87-4cb1-85d7-2ef6e561f3d8",
   "metadata": {},
   "source": [
    "**Tutorial 7.13. To implement a Random Forest algorithm on patient data to classify the blood pressure of 20 patients into `low`,`normal`,`high`**\n",
    "\n",
    "Dataset used: `patient_data.csv` Synthetic dataset generated for this book.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a38d15a-f567-41b6-b0c7-e56286a6b4bd",
   "metadata": {},
   "source": [
    "**_Syntax for implementing Random Forest Classifier with sklearn_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed843d02-43ad-49e4-91ba-32e0cfed33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create a Random Forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "# Train the classifier\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27384924-1983-427e-9bf2-0642accb0775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Read the data\n",
    "data = pd.read_csv(\n",
    "    \"/workspaces/ImplementingStatisticsWithPython/data/chapter7/patient_data.csv\")\n",
    "# Separate the features and the target\n",
    "X = data.drop(\"blood_pressure\", axis=1)  # independent variables\n",
    "y = data[\"blood_pressure\"]  # dependent variable\n",
    "# Encode the categorical features\n",
    "X[\"gender\"] = X[\"gender\"].map({\"M\": 0, \"F\": 1})\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "# Create a Random Forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "# Train the classifier\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5d804-00ab-4dd5-a495-31beec578d1d",
   "metadata": {},
   "source": [
    "**Tutorial 7.14: To evaluate Tutorial 7.13 fittted Random Forest classifier on the test set of data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aab324-fd3c-4a25-91b6-975000410d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Further test set can be used to evaluate the model\n",
    "# Predict the values for the test set\n",
    "y_pred = tree.predict(X_test)  # Get the predicted values for the test data\n",
    "# Calculate the accuracy score on the test set\n",
    "# Compare the predicted values with the actual values\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy of the Random Forest classifier model on the test set :\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a284b1b-98a1-4c7b-b4e9-317e3c2ddbdf",
   "metadata": {},
   "source": [
    "**Tutorial 7.15. To implement a Support Vector Machines (SVM), support vector classifier algorithm on patient data to classify the blood pressure of 20 patients into low, normal, high and evaluate the result**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb20403-c97a-4638-8ee8-8b0d0c1c31d3",
   "metadata": {},
   "source": [
    "**_Syntax for implementing Support vector classifier from SVM with sklearn_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346a88e-ecf1-43cc-a978-3fd5834eb6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Support vector classifier from SVM\n",
    "from sklearn.svm import SVC\n",
    "# Create a Support Vector Classifier object\n",
    "svm = SVC()\n",
    "# Train the classifier\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42b1f0-b3d5-44f6-add1-2fc97690d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Import the SVC class\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Read the data\n",
    "data = pd.read_csv(\n",
    "    \"/workspaces/ImplementingStatisticsWithPython/data/chapter7/patient_data.csv\")\n",
    "# Separate the features and the target\n",
    "X = data.drop(\"blood_pressure\", axis=1)  # independent variables\n",
    "y = data[\"blood_pressure\"]  # dependent variable\n",
    "# Encode the categorical features\n",
    "X[\"gender\"] = X[\"gender\"].map({\"M\": 0, \"F\": 1})\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "# Create an SVM classifier\n",
    "# You can change the parameters as you wish\n",
    "svm = SVC(kernel=\"rbf\", C=1, gamma=0.1)\n",
    "# Train the classifier\n",
    "svm.fit(X_train, y_train)\n",
    "# Predict the values for the test set\n",
    "y_pred = svm.predict(X_test)  # Get the predicted values for the test data\n",
    "# Calculate the accuracy score on the test set\n",
    "# Compare the predicted values with the actual values\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy of the SVM classifier model on the test set :\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa21b305-a1a4-4d4e-b838-85c49f8d84ed",
   "metadata": {},
   "source": [
    "**Tutorial 7.16. To implement a K-nearest neighbor (KNN) on iris dataset to predict the type of flower based on its features, such as petal length, petal width, sepal length, and sepal width and also evaluate the result**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac40e9-042c-4307-b3f7-1e46633d2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "# Import the KNeighborsClassifier class\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Import train_test_split for data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import accuracy_score for evaluating model performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "# Separate the features and the target variable\n",
    "# Features (sepal length, sepal width, petal length, petal width)\n",
    "X = iris.data\n",
    "# Target variable (species: Iris-setosa, Iris-versicolor, Iris-virginica)\n",
    "y = iris.target\n",
    "# Encode categorical features (if any)\n",
    "# No categorical features in the Iris dataset\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42)\n",
    "# Create a KNeighborsClassifier object\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # Set number of neighbors to 5\n",
    "# Train the classifier\n",
    "knn.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "# Evaluate the model's performance using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy of the KNN classifier on the test set :\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793f078-3aa0-408a-bf0f-8a25beab9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is Unsupervised learning ? How it works ?\n",
    "2. When or where is unsupervised learning useful, In what kind of data and situation ? \n",
    "3. Give an example to explain unsupervised learning? Use the same example to explain unsupervised learning to a eigth grader ?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f9235e4-b82a-4876-96de-49ffcad820df",
   "metadata": {},
   "source": [
    "**Tutorial 7.17: To implement Kmeans clustering using `sklearn` on a sample data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4854e1-86f8-4f19-af7e-b781dcdf55e0",
   "metadata": {},
   "source": [
    "**_Syntax for K-means using `sklearn`_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86dcabb-1ee1-459c-8787-80b2ca29bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Load the dataset\n",
    "data = ...\n",
    "# Create and fit the k-means model, n_clusters can be any number of clusters\n",
    "kmeans = KMeans(n_clusters=...)\n",
    "kmeans.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850b841-1fa3-459d-b2e9-63936490fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Load the dataset\n",
    "data = [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]]\n",
    "# Create and fit the k-means model\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(data)\n",
    "# Predict the cluster labels for each data point\n",
    "labels = kmeans.predict(data)\n",
    "print(f\"Clusters labels for data: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153d586-5284-43a3-8ec8-530f2a33980d",
   "metadata": {},
   "source": [
    "**Tutorial 7.18: To implement K-prototype using `kmodes` on a sample data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad828d41-1ebe-43a5-8517-f88023a62dc0",
   "metadata": {},
   "source": [
    "**_Syntax for K-prototype using `kmodes`_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270b0c0-a7e0-4873-abfa-8d2fe3c54f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kprototypes import KPrototypes\n",
    "# Load the dataset\n",
    "data = ...\n",
    "# Create and fit the k-prototypes model\n",
    "kproto = KPrototypes(n_clusters=3, init='Cao')\n",
    "kproto.fit(data, categorical=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686d7f9-59c6-4fe3-aca2-243090a58705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from kmodes.kmodes import KModes\n",
    "# Load the dataset\n",
    "data = [[1, 2, 'A'], [2, 3, 'B'], [3, 4, 'A'],\n",
    "        [4, 5, 'B'], [5, 6, 'B'], [6, 7, 'A']]\n",
    "# Convert the data to a NumPy array\n",
    "data = np.array(data)\n",
    "# Define the number of clusters\n",
    "num_clusters = 3\n",
    "# Create and fit the k-prototypes model\n",
    "kprototypes = KModes(n_clusters=num_clusters, init='random')\n",
    "kprototypes.fit(data)\n",
    "# Predict the cluster labels for each data point\n",
    "labels = kprototypes.predict(data)\n",
    "print(f\"Clusters labels for data: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc360e9-73b6-468a-8da3-7e0c6942de0b",
   "metadata": {},
   "source": [
    "**Tutorial 7.19: To implement hierarchical clustering using sklearn on a sample data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c4d7c-27c4-42f3-b53e-dcbeff992128",
   "metadata": {},
   "source": [
    "**_Syntax for Hierarchical clustering using `sklearn`_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef4719-f723-4e82-a618-8dca7c331e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# Load the dataset\n",
    "data = ...\n",
    "# Create and fit the hierarchical clustering model\n",
    "hier = AgglomerativeClustering(n_clusters=3)\n",
    "hier.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060406ba-040d-4a69-a1d3-06ab03ba7fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# Load the dataset\n",
    "data = [[1, 1], [1, 2], [2, 2], [2, 3], [3, 3], [3, 4]]\n",
    "# Create and fit the hierarchical clustering model\n",
    "cluster = AgglomerativeClustering(n_clusters=3)\n",
    "cluster.fit(data)\n",
    "# Predict the cluster labels for each data point\n",
    "labels = cluster.labels_\n",
    "print(f\"Clusters labels for data: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f4a5b-210e-4b6c-b970-c8de27f4b140",
   "metadata": {},
   "source": [
    "**Tutorial 7.20: To implement Gaussian mixture models using sklearn on a generated sample data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598df200-adee-4903-b396-19f1dc7c0a68",
   "metadata": {},
   "source": [
    "**_Syntax for Gaussian mixture models using `sklearn`_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0573a-0212-4caa-9609-ed742e1433c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "# Load the dataset\n",
    "data = ...\n",
    "# Create and fit the Gaussian mixture model\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed819d-1459-4b8d-bb48-b85f0cc7dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs\n",
    "# Generate some data\n",
    "X, y = make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=1.5)\n",
    "# Create a GMM with 3 components/clusters\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "# Fit the GMM to the data\n",
    "gmm.fit(X)\n",
    "# Predict the cluster labels for each data point\n",
    "labels = gmm.predict(X)\n",
    "print(f\"Clusters labels for data: {labels}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d6001a7-9ad7-47a0-ab48-33903c532061",
   "metadata": {},
   "source": [
    "**Tutorial 7.21: To implement principal component analysis using sklearn on a iris flower dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9908e9-f831-49f3-9f1a-da4f816f6c9b",
   "metadata": {},
   "source": [
    "**_Syntax for Principal component analysis using `sklearn`_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd480f47-3704-424e-9833-1478882bab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Load the dataset\n",
    "data = ...\n",
    "# Create and fit the PCA model\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32bddde-c9b6-4635-a887-c4fe70800395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "# Create a PCA model with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "# Fit the PCA model to the data\n",
    "X_pca = pca.fit_transform(X)  # Transform the data into 2 principal components\n",
    "print(\n",
    "    f\"Variance explained by principal components: {pca.explained_variance_ratio_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2646bd-993f-4fd7-ac9a-0cbde9e00827",
   "metadata": {},
   "source": [
    "**Tutorial 7.22: To implement singular value decomposition using sklearn on a iris flower dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047f1b4-3ec9-4b27-9724-d6c709e4e5ed",
   "metadata": {},
   "source": [
    "**_Syntax for Singular value decomposition using `numpy`_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63487b2-8b20-406c-a436-d4aca6ed3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "# Load the dataset\n",
    "data = ...\n",
    "# Perform the SVD\n",
    "u, s, v = svd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38023f-118d-4ce7-b0b0-d2fe7cdd6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "# Create a truncated SVD model with 2 components\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "# Fit the truncated SVD model to the data\n",
    "X_svd = svd.fit_transform(X)\n",
    "print(\n",
    "    f\"Variance explained after singular value decomposition: {svd.explained_variance_ratio_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de80a49-190a-41b7-94a9-2a6f2c0309de",
   "metadata": {},
   "source": [
    "**Tutorial 7.23: To implement DBSCAN using sklearn on a generated sample data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231cbb62-6423-474e-bbee-75e97e7a44e8",
   "metadata": {},
   "source": [
    "**_Syntax for DBSCAN using `sklearn`_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3989dd-35ff-4834-bf55-3df073c89a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "# Load the dataset\n",
    "data = ...\n",
    "# Create and fit the DBSCAN model\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39afbd3-1825-4536-b323-1af50277c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "# Generate some data\n",
    "X, y = make_moons(n_samples=200, noise=0.1)\n",
    "# Create a DBSCAN clusterer\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10)\n",
    "# Fit the DBSCAN clusterer to the data\n",
    "dbscan.fit(X)\n",
    "# Predict the cluster labels for each data point\n",
    "labels = dbscan.labels_\n",
    "print(f\"Clusters labels for data: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab51eec-b821-46ea-baae-7e0403fdb4dc",
   "metadata": {},
   "source": [
    "**Tutorial 7.24: To implement t-Distributed Stochastic Neighbor Embedding (t-SNE) using sklearn on a iris flower dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e3dc6-9a13-4cb5-ac27-87fde532d119",
   "metadata": {},
   "source": [
    "**_Syntax for t-Distributed Stochastic Neighbor Embedding (t-SNE) using `sklearn`_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1023b4a-1135-42f7-be5d-cc8fa32ab97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# Load the dataset\n",
    "data = ...\n",
    "# Create and fit the t-SNE model\n",
    "tsne = TSNE(n_components=2, perplexity=30)\n",
    "tsne.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f79d36-fcaf-4cdd-93bc-0ccbaada1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.manifold import TSNE\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "# Create a t-SNE model\n",
    "tsne = TSNE()\n",
    "# Fit the t-SNE model to the data\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "# Plot the t-SNE results\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=iris.target)\n",
    "# Define labels and colors\n",
    "labels = ['setosa', 'versicolor', 'virginica']\n",
    "colors = ['blue', 'orange', 'green']\n",
    "# Create a list of handles for the legend\n",
    "handles = [plt.plot([], [], color=c, marker='o', ls='')[0] for c in colors]\n",
    "# Add the legend to the plot\n",
    "plt.legend(handles, labels, loc='upper right')\n",
    "# x and y labels\n",
    "plt.xlabel('t-SNE dimension 1')\n",
    "plt.ylabel('t-SNE dimension 2')\n",
    "# Title\n",
    "plt.title('t-SNE visualization of the Iris dataset')\n",
    "# Show the figure\n",
    "plt.savefig('TSNE.jpg', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0442fde1-1fd7-4614-99f0-5fc75d0c9e6d",
   "metadata": {},
   "source": [
    "**Apriori**\n",
    "\n",
    "Dataset used: Groceries Dataset from the UCI Machine Learning Repository\n",
    "\n",
    "From : https://archive.ics.uci.edu/dataset/292/wholesale+customers\n",
    "\n",
    "Licence: Creative Commons Attribution 4.0 International license (CC-BY 4.0)\n",
    "\n",
    "Install: `pip install apyori`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556980bf",
   "metadata": {},
   "source": [
    "**Tutorial 7.25: To implement Apriori using `apyori` to find the all the freqently bough item from a grocery item dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d395b-56ac-4b4e-9265-9d01a4618fc0",
   "metadata": {},
   "source": [
    "**_Syntax for Apriori using `apyori`_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9661fb-644b-458a-b633-2cdde385a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "# Load the dataset\n",
    "data = ...\n",
    "# Create and fit the apriori model\n",
    "rules = apriori(data, min_support=0.01, min_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79ebced4-f690-492d-aca7-da34093ace14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Detergents_Paper']\n",
      "['Fresh']\n",
      "['Frozen']\n",
      "['Grocery']\n",
      "['Milk']\n",
      "['Region']\n",
      "['Detergents_Paper', 'Delicassen']\n",
      "['Delicassen', 'Fresh']\n",
      "['Frozen', 'Delicassen']\n",
      "['Delicassen', 'Grocery']\n",
      "['Delicassen', 'Milk']\n",
      "['Delicassen', 'Region']\n",
      "['Detergents_Paper', 'Fresh']\n",
      "['Frozen', 'Detergents_Paper']\n",
      "['Detergents_Paper', 'Grocery']\n",
      "['Detergents_Paper', 'Milk']\n",
      "['Detergents_Paper', 'Region']\n",
      "['Frozen', 'Fresh']\n",
      "['Fresh', 'Grocery']\n",
      "['Fresh', 'Milk']\n",
      "['Fresh', 'Region']\n",
      "['Frozen', 'Grocery']\n",
      "['Frozen', 'Milk']\n",
      "['Frozen', 'Region']\n",
      "['Milk', 'Grocery']\n",
      "['Region', 'Grocery']\n",
      "['Region', 'Milk']\n",
      "['Detergents_Paper', 'Delicassen', 'Fresh']\n",
      "['Frozen', 'Detergents_Paper', 'Delicassen']\n",
      "['Detergents_Paper', 'Delicassen', 'Grocery']\n",
      "['Detergents_Paper', 'Delicassen', 'Milk']\n",
      "['Detergents_Paper', 'Delicassen', 'Region']\n",
      "['Frozen', 'Delicassen', 'Fresh']\n",
      "['Delicassen', 'Fresh', 'Grocery']\n",
      "['Delicassen', 'Fresh', 'Milk']\n",
      "['Delicassen', 'Fresh', 'Region']\n",
      "['Frozen', 'Delicassen', 'Grocery']\n",
      "['Frozen', 'Delicassen', 'Milk']\n",
      "['Frozen', 'Delicassen', 'Region']\n",
      "['Delicassen', 'Milk', 'Grocery']\n",
      "['Delicassen', 'Region', 'Grocery']\n",
      "['Delicassen', 'Region', 'Milk']\n",
      "['Frozen', 'Detergents_Paper', 'Fresh']\n",
      "['Detergents_Paper', 'Fresh', 'Grocery']\n",
      "['Detergents_Paper', 'Fresh', 'Milk']\n",
      "['Detergents_Paper', 'Fresh', 'Region']\n",
      "['Frozen', 'Detergents_Paper', 'Grocery']\n",
      "['Frozen', 'Detergents_Paper', 'Milk']\n",
      "['Frozen', 'Detergents_Paper', 'Region']\n",
      "['Detergents_Paper', 'Milk', 'Grocery']\n",
      "['Detergents_Paper', 'Region', 'Grocery']\n",
      "['Detergents_Paper', 'Region', 'Milk']\n",
      "['Frozen', 'Fresh', 'Grocery']\n",
      "['Frozen', 'Fresh', 'Milk']\n",
      "['Frozen', 'Fresh', 'Region']\n",
      "['Fresh', 'Milk', 'Grocery']\n",
      "['Fresh', 'Region', 'Grocery']\n",
      "['Region', 'Fresh', 'Milk']\n",
      "['Frozen', 'Milk', 'Grocery']\n",
      "['Frozen', 'Region', 'Grocery']\n",
      "['Frozen', 'Region', 'Milk']\n",
      "['Region', 'Milk', 'Grocery']\n",
      "['Frozen', 'Detergents_Paper', 'Delicassen', 'Fresh']\n",
      "['Detergents_Paper', 'Delicassen', 'Fresh', 'Grocery']\n",
      "['Detergents_Paper', 'Delicassen', 'Fresh', 'Milk']\n",
      "['Detergents_Paper', 'Delicassen', 'Fresh', 'Region']\n",
      "['Frozen', 'Detergents_Paper', 'Delicassen', 'Grocery']\n",
      "['Frozen', 'Detergents_Paper', 'Delicassen', 'Milk']\n",
      "['Frozen', 'Detergents_Paper', 'Delicassen', 'Region']\n",
      "['Detergents_Paper', 'Delicassen', 'Milk', 'Grocery']\n",
      "['Detergents_Paper', 'Delicassen', 'Region', 'Grocery']\n",
      "['Detergents_Paper', 'Delicassen', 'Region', 'Milk']\n",
      "['Frozen', 'Delicassen', 'Fresh', 'Grocery']\n",
      "['Frozen', 'Delicassen', 'Fresh', 'Milk']\n",
      "['Frozen', 'Delicassen', 'Fresh', 'Region']\n",
      "['Delicassen', 'Fresh', 'Milk', 'Grocery']\n",
      "['Delicassen', 'Fresh', 'Region', 'Grocery']\n",
      "['Region', 'Delicassen', 'Fresh', 'Milk']\n",
      "['Frozen', 'Delicassen', 'Milk', 'Grocery']\n",
      "['Frozen', 'Delicassen', 'Region', 'Grocery']\n",
      "['Frozen', 'Delicassen', 'Region', 'Milk']\n",
      "['Delicassen', 'Region', 'Milk', 'Grocery']\n",
      "['Frozen', 'Detergents_Paper', 'Fresh', 'Grocery']\n",
      "['Frozen', 'Detergents_Paper', 'Fresh', 'Milk']\n",
      "['Frozen', 'Detergents_Paper', 'Fresh', 'Region']\n",
      "['Detergents_Paper', 'Fresh', 'Milk', 'Grocery']\n",
      "['Detergents_Paper', 'Fresh', 'Region', 'Grocery']\n",
      "['Region', 'Detergents_Paper', 'Fresh', 'Milk']\n",
      "['Frozen', 'Detergents_Paper', 'Milk', 'Grocery']\n",
      "['Frozen', 'Detergents_Paper', 'Region', 'Grocery']\n",
      "['Frozen', 'Detergents_Paper', 'Region', 'Milk']\n",
      "['Detergents_Paper', 'Region', 'Milk', 'Grocery']\n",
      "['Frozen', 'Fresh', 'Milk', 'Grocery']\n",
      "['Frozen', 'Fresh', 'Region', 'Grocery']\n",
      "['Frozen', 'Region', 'Fresh', 'Milk']\n",
      "['Region', 'Fresh', 'Milk', 'Grocery']\n",
      "['Frozen', 'Region', 'Milk', 'Grocery']\n",
      "['Grocery', 'Delicassen', 'Fresh', 'Frozen', 'Detergents_Paper']\n",
      "['Delicassen', 'Fresh', 'Milk', 'Frozen', 'Detergents_Paper']\n",
      "['Delicassen', 'Fresh', 'Region', 'Frozen', 'Detergents_Paper']\n",
      "['Grocery', 'Delicassen', 'Fresh', 'Milk', 'Detergents_Paper']\n",
      "['Grocery', 'Delicassen', 'Fresh', 'Region', 'Detergents_Paper']\n",
      "['Delicassen', 'Fresh', 'Region', 'Milk', 'Detergents_Paper']\n",
      "['Grocery', 'Delicassen', 'Milk', 'Frozen', 'Detergents_Paper']\n",
      "['Grocery', 'Delicassen', 'Region', 'Frozen', 'Detergents_Paper']\n",
      "['Delicassen', 'Region', 'Milk', 'Frozen', 'Detergents_Paper']\n",
      "['Grocery', 'Delicassen', 'Region', 'Milk', 'Detergents_Paper']\n",
      "['Grocery', 'Delicassen', 'Fresh', 'Milk', 'Frozen']\n",
      "['Grocery', 'Delicassen', 'Fresh', 'Region', 'Frozen']\n",
      "['Delicassen', 'Fresh', 'Region', 'Milk', 'Frozen']\n",
      "['Grocery', 'Delicassen', 'Fresh', 'Region', 'Milk']\n",
      "['Grocery', 'Delicassen', 'Region', 'Milk', 'Frozen']\n",
      "['Grocery', 'Fresh', 'Milk', 'Frozen', 'Detergents_Paper']\n",
      "['Grocery', 'Fresh', 'Region', 'Frozen', 'Detergents_Paper']\n",
      "['Fresh', 'Region', 'Milk', 'Frozen', 'Detergents_Paper']\n",
      "['Grocery', 'Fresh', 'Region', 'Milk', 'Detergents_Paper']\n",
      "['Grocery', 'Region', 'Milk', 'Frozen', 'Detergents_Paper']\n",
      "['Grocery', 'Fresh', 'Region', 'Milk', 'Frozen']\n",
      "['Grocery', 'Delicassen', 'Fresh', 'Milk', 'Frozen', 'Detergents_Paper']\n",
      "['Grocery', 'Delicassen', 'Fresh', 'Region', 'Frozen', 'Detergents_Paper']\n",
      "['Delicassen', 'Fresh', 'Milk', 'Region', 'Frozen', 'Detergents_Paper']\n",
      "['Grocery', 'Delicassen', 'Fresh', 'Milk', 'Region', 'Detergents_Paper']\n",
      "['Grocery', 'Delicassen', 'Milk', 'Region', 'Frozen', 'Detergents_Paper']\n",
      "['Grocery', 'Delicassen', 'Fresh', 'Milk', 'Region', 'Frozen']\n",
      "['Grocery', 'Fresh', 'Milk', 'Region', 'Frozen', 'Detergents_Paper']\n",
      "['Grocery', 'Delicassen', 'Fresh', 'Milk', 'Region', 'Frozen', 'Detergents_Paper']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from apyori import apriori\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\n",
    "    '/workspaces/ImplementingStatisticsWithPython/data/chapter7/Groceries.csv')\n",
    "# Reshape the data from wide to long format\n",
    "data = pd.melt(data, id_vars='Channel',\n",
    "               var_name='Product', value_name='Quantity')\n",
    "# Group the data by customer and aggregate the product categories into a list\n",
    "data = data.groupby('Channel')['Product'].apply(list)\n",
    "# Convert the data into a list of lists\n",
    "data = data.tolist()\n",
    "# Create the apriori model\n",
    "rules = apriori(data, min_support=0.00003)\n",
    "# Print the rules\n",
    "for rule in rules:\n",
    "    for rule in rules:\n",
    "        print(list(rule.items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75319a15-404e-451f-a328-f40c8187d6a4",
   "metadata": {},
   "source": [
    "**Tutorial 7.26: To implement Apriori using `apyori` to find view only the the first 5 frequent items from a grocery item dataset.**\n",
    "\n",
    "Here we use python slicing concept to limit the output of the Apriori algorithm to the first 5 frequent items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3e4e422-f825-4fb5-8a56-0e341f465647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delicassen\n",
      "Detergents_Paper\n",
      "Fresh\n",
      "Frozen\n",
      "Grocery\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from apyori import apriori\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\n",
    "    '/workspaces/ImplementingStatisticsWithPython/data/chapter7/Groceries.csv')\n",
    "# Reshape the data from wide to long format\n",
    "data = pd.melt(data, id_vars='Channel',\n",
    "               var_name='Product', value_name='Quantity')\n",
    "# Group the data by customer and aggregate the product categories into a list\n",
    "data = data.groupby('Channel')['Product'].apply(list)\n",
    "# Convert the data into a list of lists\n",
    "data = data.tolist()\n",
    "# Create the apriori model\n",
    "rules = apriori(data, min_support=0.00003)\n",
    "# Print the rules and the first 5 elements\n",
    "rules = list(rules)  \n",
    "rules = rules[:5]  \n",
    "for rule in rules:\n",
    "    for item in rule.items:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebe8d7-a42d-4776-b7b7-bc29a7a1ab29",
   "metadata": {},
   "source": [
    "**Tutorial 7.27: To implement Apriori using `apyori` to view all most frequent items with the support value of each itemset from the grocery item dataset.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc352662-f8d8-4251-8063-0818611815e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delicassen: 1.0\n",
      "Detergents_Paper: 1.0\n",
      "Fresh: 1.0\n",
      "Frozen: 1.0\n",
      "Grocery: 1.0\n",
      "Milk: 1.0\n",
      "Region: 1.0\n",
      "Detergents_Paper, Delicassen: 1.0\n",
      "Delicassen, Fresh: 1.0\n",
      "Frozen, Delicassen: 1.0\n",
      "Delicassen, Grocery: 1.0\n",
      "Delicassen, Milk: 1.0\n",
      "Delicassen, Region: 1.0\n",
      "Detergents_Paper, Fresh: 1.0\n",
      "Frozen, Detergents_Paper: 1.0\n",
      "Detergents_Paper, Grocery: 1.0\n",
      "Detergents_Paper, Milk: 1.0\n",
      "Detergents_Paper, Region: 1.0\n",
      "Frozen, Fresh: 1.0\n",
      "Fresh, Grocery: 1.0\n",
      "Fresh, Milk: 1.0\n",
      "Fresh, Region: 1.0\n",
      "Frozen, Grocery: 1.0\n",
      "Frozen, Milk: 1.0\n",
      "Frozen, Region: 1.0\n",
      "Milk, Grocery: 1.0\n",
      "Region, Grocery: 1.0\n",
      "Region, Milk: 1.0\n",
      "Detergents_Paper, Delicassen, Fresh: 1.0\n",
      "Frozen, Detergents_Paper, Delicassen: 1.0\n",
      "Detergents_Paper, Delicassen, Grocery: 1.0\n",
      "Detergents_Paper, Delicassen, Milk: 1.0\n",
      "Detergents_Paper, Delicassen, Region: 1.0\n",
      "Frozen, Delicassen, Fresh: 1.0\n",
      "Delicassen, Fresh, Grocery: 1.0\n",
      "Delicassen, Fresh, Milk: 1.0\n",
      "Delicassen, Fresh, Region: 1.0\n",
      "Frozen, Delicassen, Grocery: 1.0\n",
      "Frozen, Delicassen, Milk: 1.0\n",
      "Frozen, Delicassen, Region: 1.0\n",
      "Delicassen, Milk, Grocery: 1.0\n",
      "Delicassen, Region, Grocery: 1.0\n",
      "Delicassen, Region, Milk: 1.0\n",
      "Frozen, Detergents_Paper, Fresh: 1.0\n",
      "Detergents_Paper, Fresh, Grocery: 1.0\n",
      "Detergents_Paper, Fresh, Milk: 1.0\n",
      "Detergents_Paper, Fresh, Region: 1.0\n",
      "Frozen, Detergents_Paper, Grocery: 1.0\n",
      "Frozen, Detergents_Paper, Milk: 1.0\n",
      "Frozen, Detergents_Paper, Region: 1.0\n",
      "Detergents_Paper, Milk, Grocery: 1.0\n",
      "Detergents_Paper, Region, Grocery: 1.0\n",
      "Detergents_Paper, Region, Milk: 1.0\n",
      "Frozen, Fresh, Grocery: 1.0\n",
      "Frozen, Fresh, Milk: 1.0\n",
      "Frozen, Fresh, Region: 1.0\n",
      "Fresh, Milk, Grocery: 1.0\n",
      "Fresh, Region, Grocery: 1.0\n",
      "Region, Fresh, Milk: 1.0\n",
      "Frozen, Milk, Grocery: 1.0\n",
      "Frozen, Region, Grocery: 1.0\n",
      "Frozen, Region, Milk: 1.0\n",
      "Region, Milk, Grocery: 1.0\n",
      "Frozen, Detergents_Paper, Delicassen, Fresh: 1.0\n",
      "Detergents_Paper, Delicassen, Fresh, Grocery: 1.0\n",
      "Detergents_Paper, Delicassen, Fresh, Milk: 1.0\n",
      "Detergents_Paper, Delicassen, Fresh, Region: 1.0\n",
      "Frozen, Detergents_Paper, Delicassen, Grocery: 1.0\n",
      "Frozen, Detergents_Paper, Delicassen, Milk: 1.0\n",
      "Frozen, Detergents_Paper, Delicassen, Region: 1.0\n",
      "Detergents_Paper, Delicassen, Milk, Grocery: 1.0\n",
      "Detergents_Paper, Delicassen, Region, Grocery: 1.0\n",
      "Detergents_Paper, Delicassen, Region, Milk: 1.0\n",
      "Frozen, Delicassen, Fresh, Grocery: 1.0\n",
      "Frozen, Delicassen, Fresh, Milk: 1.0\n",
      "Frozen, Delicassen, Fresh, Region: 1.0\n",
      "Delicassen, Fresh, Milk, Grocery: 1.0\n",
      "Delicassen, Fresh, Region, Grocery: 1.0\n",
      "Region, Delicassen, Fresh, Milk: 1.0\n",
      "Frozen, Delicassen, Milk, Grocery: 1.0\n",
      "Frozen, Delicassen, Region, Grocery: 1.0\n",
      "Frozen, Delicassen, Region, Milk: 1.0\n",
      "Delicassen, Region, Milk, Grocery: 1.0\n",
      "Frozen, Detergents_Paper, Fresh, Grocery: 1.0\n",
      "Frozen, Detergents_Paper, Fresh, Milk: 1.0\n",
      "Frozen, Detergents_Paper, Fresh, Region: 1.0\n",
      "Detergents_Paper, Fresh, Milk, Grocery: 1.0\n",
      "Detergents_Paper, Fresh, Region, Grocery: 1.0\n",
      "Region, Detergents_Paper, Fresh, Milk: 1.0\n",
      "Frozen, Detergents_Paper, Milk, Grocery: 1.0\n",
      "Frozen, Detergents_Paper, Region, Grocery: 1.0\n",
      "Frozen, Detergents_Paper, Region, Milk: 1.0\n",
      "Detergents_Paper, Region, Milk, Grocery: 1.0\n",
      "Frozen, Fresh, Milk, Grocery: 1.0\n",
      "Frozen, Fresh, Region, Grocery: 1.0\n",
      "Frozen, Region, Fresh, Milk: 1.0\n",
      "Region, Fresh, Milk, Grocery: 1.0\n",
      "Frozen, Region, Milk, Grocery: 1.0\n",
      "Grocery, Delicassen, Fresh, Frozen, Detergents_Paper: 1.0\n",
      "Delicassen, Fresh, Milk, Frozen, Detergents_Paper: 1.0\n",
      "Delicassen, Fresh, Region, Frozen, Detergents_Paper: 1.0\n",
      "Grocery, Delicassen, Fresh, Milk, Detergents_Paper: 1.0\n",
      "Grocery, Delicassen, Fresh, Region, Detergents_Paper: 1.0\n",
      "Delicassen, Fresh, Region, Milk, Detergents_Paper: 1.0\n",
      "Grocery, Delicassen, Milk, Frozen, Detergents_Paper: 1.0\n",
      "Grocery, Delicassen, Region, Frozen, Detergents_Paper: 1.0\n",
      "Delicassen, Region, Milk, Frozen, Detergents_Paper: 1.0\n",
      "Grocery, Delicassen, Region, Milk, Detergents_Paper: 1.0\n",
      "Grocery, Delicassen, Fresh, Milk, Frozen: 1.0\n",
      "Grocery, Delicassen, Fresh, Region, Frozen: 1.0\n",
      "Delicassen, Fresh, Region, Milk, Frozen: 1.0\n",
      "Grocery, Delicassen, Fresh, Region, Milk: 1.0\n",
      "Grocery, Delicassen, Region, Milk, Frozen: 1.0\n",
      "Grocery, Fresh, Milk, Frozen, Detergents_Paper: 1.0\n",
      "Grocery, Fresh, Region, Frozen, Detergents_Paper: 1.0\n",
      "Fresh, Region, Milk, Frozen, Detergents_Paper: 1.0\n",
      "Grocery, Fresh, Region, Milk, Detergents_Paper: 1.0\n",
      "Grocery, Region, Milk, Frozen, Detergents_Paper: 1.0\n",
      "Grocery, Fresh, Region, Milk, Frozen: 1.0\n",
      "Grocery, Delicassen, Fresh, Milk, Frozen, Detergents_Paper: 1.0\n",
      "Grocery, Delicassen, Fresh, Region, Frozen, Detergents_Paper: 1.0\n",
      "Delicassen, Fresh, Milk, Region, Frozen, Detergents_Paper: 1.0\n",
      "Grocery, Delicassen, Fresh, Milk, Region, Detergents_Paper: 1.0\n",
      "Grocery, Delicassen, Milk, Region, Frozen, Detergents_Paper: 1.0\n",
      "Grocery, Delicassen, Fresh, Milk, Region, Frozen: 1.0\n",
      "Grocery, Fresh, Milk, Region, Frozen, Detergents_Paper: 1.0\n",
      "Grocery, Delicassen, Fresh, Milk, Region, Frozen, Detergents_Paper: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from apyori import apriori\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\n",
    "    '/workspaces/ImplementingStatisticsWithPython/data/chapter7/Groceries.csv')\n",
    "# Reshape the data from wide to long format\n",
    "data = pd.melt(data, id_vars='Channel',\n",
    "               var_name='Product', value_name='Quantity')\n",
    "# Group the data by customer and aggregate the product categories into a list\n",
    "data = data.groupby('Channel')['Product'].apply(list)\n",
    "# Convert the data into a list of lists\n",
    "data = data.tolist()\n",
    "# Create the apriori model\n",
    "rules = apriori(data, min_support=0.00003)\n",
    "# Print the rules\n",
    "for rule in rules:\n",
    "    # Join the items in the itemset with a comma\n",
    "    itemset = \", \".join(rule.items)\n",
    "    # Get the support value of the itemset\n",
    "    support = rule.support\n",
    "    # Print the itemset and the support in one line\n",
    "    print(\"{}: {}\".format(itemset, support))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce932d9e-407d-4a06-8bd2-20ddaa681414",
   "metadata": {},
   "source": [
    "**Tutorial 7.28: To implement frequent item data minning using a sample data set of transactions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc451f8-da41-4a5c-a6ab-584dab8a2596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 4\n",
      "B 4\n",
      "C 5\n",
      "D 3\n",
      "E 3\n",
      "AB 3\n",
      "AC 4\n",
      "AE 3\n",
      "BC 4\n",
      "BD 3\n",
      "CD 3\n",
      "CE 3\n",
      "ABC 3\n",
      "ACE 3\n",
      "BCD 3\n"
     ]
    }
   ],
   "source": [
    "# Define a function to convert the data from horizontal to vertical format\n",
    "def horizontal_to_vertical(data):\n",
    "  # Initialize an empty dictionary to store the vertical format\n",
    "  vertical = {}\n",
    "  # Loop through each transaction in the data\n",
    "  for i, transaction in enumerate(data):\n",
    "    # Loop through each item in the transaction\n",
    "    for item in transaction:\n",
    "      # If the item is already in the dictionary, append the transaction ID to its value\n",
    "      if item in vertical:\n",
    "        vertical[item].append(i)\n",
    "      # Otherwise, create a new key-value pair with the item and the transaction ID\n",
    "      else:\n",
    "        vertical[item] = [i]\n",
    "  # Return the vertical format\n",
    "  return vertical\n",
    "\n",
    "# Define a function to generate frequent item sets using the ECLAT algorithm\n",
    "def eclat(data, min_support):\n",
    "  # Convert the data to vertical format\n",
    "  vertical = horizontal_to_vertical(data)\n",
    "  # Initialize an empty list to store the frequent item sets\n",
    "  frequent = []\n",
    "  # Initialize an empty list to store the candidates\n",
    "  candidates = []\n",
    "  # Loop through each item in the vertical format\n",
    "  for item in vertical:\n",
    "    # Get the support count of the item by taking the length of its value\n",
    "    support = len(vertical[item])\n",
    "    # If the support count is greater than or equal to the minimum support, add the item to the frequent list and the candidates list\n",
    "    if support >= min_support:\n",
    "      frequent.append((item, support))\n",
    "      candidates.append((item, vertical[item]))\n",
    "  # Loop until there are no more candidates\n",
    "  while candidates:\n",
    "    # Initialize an empty list to store the new candidates\n",
    "    new_candidates = []\n",
    "    # Loop through each pair of candidates\n",
    "    for i in range(len(candidates) - 1):\n",
    "      for j in range(i + 1, len(candidates)):\n",
    "        # Get the first item set and its transaction IDs from the first candidate\n",
    "        itemset1, tidset1 = candidates[i]\n",
    "        # Get the second item set and its transaction IDs from the second candidate\n",
    "        itemset2, tidset2 = candidates[j]\n",
    "        # If the item sets have the same prefix, they can be combined\n",
    "        if itemset1[:-1] == itemset2[:-1]:\n",
    "          # Combine the item sets by adding the last element of the second item set to the first item set\n",
    "          new_itemset = itemset1 + itemset2[-1]\n",
    "          # Intersect the transaction IDs to get the support count of the new item set\n",
    "          new_tidset = list(set(tidset1) & set(tidset2))\n",
    "          new_support = len(new_tidset)\n",
    "          # If the support count is greater than or equal to the minimum support, add the new item set to the frequent list and the new candidates list\n",
    "          if new_support >= min_support:\n",
    "            frequent.append((new_itemset, new_support))\n",
    "            new_candidates.append((new_itemset, new_tidset))\n",
    "    # Update the candidates list with the new candidates\n",
    "    candidates = new_candidates\n",
    "  # Return the frequent item sets\n",
    "  return frequent\n",
    "\n",
    "# Define a sample data set of transactions\n",
    "data = [\n",
    "  [\"A\", \"B\", \"C\", \"D\"],\n",
    "  [\"A\", \"C\", \"E\"],\n",
    "  [\"A\", \"B\", \"C\", \"E\"],\n",
    "  [\"B\", \"C\", \"D\"],\n",
    "  [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "]\n",
    "\n",
    "# Define a minimum support value\n",
    "min_support = 3\n",
    "# Call the eclat function with the data and the minimum support\n",
    "frequent = eclat(data, min_support)\n",
    "# Print the frequent item sets and their support counts\n",
    "for itemset, support in frequent:\n",
    "  print(itemset, support)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c571b-92b4-4c43-86b5-949b96e21dbd",
   "metadata": {},
   "source": [
    "**Tutorial 7.29: To implement frequent item for data minning using FP-Growth using `mlxtend.frequent_patterns`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897616c1-c5ab-46ce-8177-9a3e820c0331",
   "metadata": {},
   "source": [
    "**Syntax for FP-Growth using `mlxtend.frequent_patterns`**\n",
    "\n",
    "Package used: `pip install mlxtend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a43aea-005b-4a5a-9cac-abf535d53f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "# Load the dataset\n",
    "data = ...\n",
    "# Create and fit the FP-Growth model\n",
    "patterns = fpgrowth(data, min_support=0.01, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b8b25d6-2b18-4faf-b7ee-b8c4bf8f6002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(C)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8</td>\n",
       "      <td>(B)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8</td>\n",
       "      <td>(A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8</td>\n",
       "      <td>(B, C)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8</td>\n",
       "      <td>(A, C)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   support itemsets\n",
       "0      1.0      (C)\n",
       "1      0.8      (B)\n",
       "2      0.8      (A)\n",
       "3      0.8   (B, C)\n",
       "4      0.8   (A, C)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Import fpgrowth function from mlxtend library for frequent pattern mining\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "# Import TransactionEncoder class from mlxtend library for encoding data\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "# Define a list of transactions, each transaction is a list of items\n",
    "data = [[\"A\", \"B\", \"C\", \"D\"],\n",
    "  [\"A\", \"C\", \"E\"],\n",
    "  [\"A\", \"B\", \"C\", \"E\"],\n",
    "  [\"B\", \"C\", \"D\"],\n",
    "  [\"A\", \"B\", \"C\", \"D\", \"E\"]]\n",
    "# Create an instance of TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "# Fit and transform the data to get a boolean matrix\n",
    "te_ary = te.fit(data).transform(data)\n",
    "# Convert the matrix to a pandas dataframe with column names as items\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "# Apply fpgrowth algorithm on the dataframe with a minimum support of 0.8\n",
    "# and return the frequent itemsets with their corresponding support values\n",
    "fpgrowth(df, min_support=0.8, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6dcb5c-04cf-4606-a693-1109de126983",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tutorial 7.30: Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d9e02-08df-4f4b-8f2b-0308d7b5470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tutorial 7.31: Evaluation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
